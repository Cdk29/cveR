---
title: "Go further"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{go-further}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(cveR)
```

<!-- WARNING - This vignette is generated by {fusen} from dev/flat_data_viz_cve.Rmd: do not edit by hand -->

# data_viz_cve

```{r}
library(doc2vec)
library(tokenizers.bpe)
library(udpipe)
library(cveR)
library(stringr)
```

```{r}
#cves_harvesting()
#df <- read_data_CVEs("allitems.csv")
```

```{r}
# # Use function
# filtered_data <- filter_data(df)
# 
# # Check dimensions
# dim(filtered_data)
# 
# # Write data to CSV
# write.csv(filtered_data, "filtered_data_18_2023.csv", row.names = FALSE)
```

The line below solve a lot of problems with the encoding of the CVEs.

```{r}
df <- read.csv("filtered_data_18_2023.csv")
df$Description <- iconv(df$Description, from = "ISO-8859-1", to = "UTF-8")
```

```{r}
# Create a new dataframe
archived_cve <- data.frame(Name = df$Name,
                     Project = "new_cves",
                     Archived = "Yes")

# Check the new dataframe
# write.csv(archived_cve, "archived_data_18_2023.csv", row.names = FALSE)
```

```{r}
library(BTM)
library(udpipe)
```

```{r}
# dl <- udpipe_download_model(language = "english")
ud_model <- udpipe_load_model("english-ewt-ud-2.5-191206.udpipe")

test_df <- head(df)
ud_data <- udpipe_annotate(ud_model, x = test_df$Description, tagger = "default", parser = "none")
ud_data <- as.data.frame(ud_data)
```

```{r}
test_df <- head(df, 600)
```

```{r examples-text_annotation}
u_df <- text_annotation(test_df, "Description", ud_model)
head(u_df)
```

```{r}
#' @title BTM Topic Modeling
#' @description This function applies the Biterm Topic Model (BTM) to a given annotated dataframe.
#' @param df Annotated dataframe containing the text data.
#' @param k Number of topics.
#' @param iter Number of iterations for the Gibbs sampling algorithm.
#' @return A BTM model.
#' @examples
#' data <- data.frame(doc_id = c(1,2,3), upos = c("NOUN", "PROPN", "NOUN"), lemma = c("noun1", "noun2", "noun3"))
#' model <- btm_topic_model(data, k = 5, iter = 1000)
btm_topic_model <- function(df, k = 10, iter = 1000) {
  
  
  # Filter the data to include only nouns (NN, NNP, NNS)
  # df <- subset(df, upos %in% c("PROPN", "VERB"))
  df <- subset(df, xpos %in% c("NN", "NNP", "NNS", "VB", "VBD", "VBG", "VBN", "VBP", "VBZ"))  
  # Prepare the data for the BTM model
  data <- df[, c("doc_id", "token")]

  # Apply the BTM model
  model <- BTM(data, k = k, iter = iter, background = TRUE)
  
  return(model)
}

```

```{r}
btm_model <- btm_topic_model(u_df)
```

```{r}
topicterms <- terms(btm_model, top_n = 10)
topicterms
```

```{r}
library(textplot)
library(ggraph)
library(concaveman)
plot(btm_model)
```

```{r}
library(data.table)
biterms <- as.data.table(u_df)
biterms <- biterms[, cooccurrence(x = token,
                                  relevant = xpos %in% c("NN", "NNP", "NNS", "VB", "VBD", "VBG", "VBN", "VBP", "VBZ") & 
                                             nchar(token) > 2 & !token %in% stopwords("en"),
                                  skipgram = 3),
                   by = list(doc_id)]
```

```{r}
traindata <- subset(u_df, xpos %in% c("NN", "NNP", "NNS", "VB", "VBD", "VBG", "VBN", "VBP", "VBZ") & !token %in% stopwords("en") & nchar(token) > 2)
traindata <- traindata[, c("doc_id", "token")]
model     <- BTM(traindata, biterms = biterms, k = 10, iter = 2000, background = TRUE)
```

```{r}
plot(model)
```

```{r examples-btm_topic_model_cooccurrence}
library(stopwords)
# Read data from CSV
df <- read.csv("filtered_data_18_2023.csv")
df <- tail(df, 600)
df$Description <- iconv(df$Description, from = "ISO-8859-1", to = "UTF-8")
# Load the udpipe model
ud_model <- udpipe_load_model("english-ewt-ud-2.5-191206.udpipe")

# Annotate the text data
u_df <- text_annotation(df, "Description", ud_model)
# Run Biterm Topic Model with cooccurrence
btm_model_cooccurrence <- btm_topic_model_cooccurrence(u_df, k = 10)

# Plot the topic model
plot(btm_model_cooccurrence)


```

